---
title: "Lab 4 - The TidyModels Package"
subtitle: "BSMM 8740 Fall 2024"
author: "THI BAO HAN TRUONG"
format: html
editor: visual
self-contained: true
reference-location: margin
---

## Introduction

In today's lab, you'll practice building `workflowsets` with `recipes`, `parsnip` models, `rsample` cross validations, model tuning and model comparison.

### Learning goals

By the end of the lab you will...

-   Be able to build workflows to evaluate different models and feature sets.

## Getting started

-   Log in to **your** github account and then go to the [GitHub organization](https://github.com/bsmm-8740-fall-2024) for the course and find the **2024-lab-4-\[your github username\]** repository to complete the lab.

    Create an R project using your **2024-lab-4-\[your github username\]** repository (remember to create a PAT, etc., as in lab-1) and add your answers by editing the `2024-lab-4.qmd` file in your repository.

-   When you are done, be sure to: **save** your document, **stage**, **commit** and [**push**]{.underline} your work.

::: callout-important
To access Github from the lab, you will need to make sure you are logged in as follows:

-   username: **.\\daladmin**
-   password: **Business507!**

Remember to (create a PAT and set your git credentials)

-   create your PAT using `usethis::create_github_token()` ,
-   store your PAT with `gitcreds::gitcreds_set()` ,
-   set your username and email with
    -   `usethis::use_git_config( user.name = ___, user.email = ___)`
:::

## Packages

```{r}
#| message: false
# check if 'librarian' is installed and if not, install it
if (! "librarian" %in% rownames(installed.packages()) ){
  install.packages("librarian")
}
  
# load packages if not already loaded
librarian::shelf(
  tidyverse, magrittr, tidymodels, modeldata, ranger, rsample, broom, recipes, parsnip, glmnet
)

# set the efault theme for plotting
theme_set(theme_bw(base_size = 18) + theme(legend.position = "top"))
```

## The Data

Today we will be using the Ames Housing Data.

This is a data set from [De Cock](http://jse.amstat.org/v19n3/decock.pdf) (2011) has 82 fields were recorded for 2,930 properties in Ames Iowa in the US. The version in the `modeldata` package is copied from the `AmesHousing` package but does not include a few quality columns that appear to be outcomes rather than predictors.

```{r}
#| eval: false
dat <- modeldata::ames
```

The data dictionary can be found on the internet:

```{r}
#| eval: false
cat(readr::read_file("http://jse.amstat.org/v19n3/decock/DataDocumentation.txt"))
```

## Exercise 1: EDA

Write and execute the code to perform summary EDA on the Ames Housing data using the package `skimr`.

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
# PLEASE SHOW YOUR WORK

skimr::skim(dat)

```
:::

## Exercise 2: Train / Test Splits

Write and execute code to create training and test datasets. Have the training dataset represent 75% of the total data.

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
#| eval: false
# PLEASE SHOW YOUR WORK

set.seed(8740)
data_split <- rsample::initial_split(ames, prop = 0.75)
data_split

ames_train <- rsample::training(data_split)
ames_test  <- rsample::testing(data_split)

```
:::

## Exercise 3: Data Preprocessing

create a recipe based on the formula **Sale_Price \~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold** with the following steps:

-   transform the variable `Sale_Price` to `log(Sale_Price)`
-   center and scale all predictors
-   create dummy variables for all nominal variables
-   transform the variable `Neighborhood` to pool infrequent values (see `recipes::step_other`)

Finally prep the recipe.

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
#| eval: false
# PLEASE SHOW YOUR WORK
norm_recipe <- 
  recipes::recipe(Sale_Price ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold, data = ames_train) %>% 
  step_log(Sale_Price) %>%
  step_center(Longitude, Latitude, Lot_Area, Year_Sold) %>%
  step_scale(Longitude, Latitude, Lot_Area, Year_Sold) %>%
  step_other(Neighborhood) %>% 
  step_dummy(all_nominal_predictors())

prep <- recipes::prep(norm_recipe, training = ames_train) %>% broom::tidy()
```
:::

## Exercise 4 Modeling

Create three regression models

-   a base regression model using `lm`

-   a regression model using `glmnet`; set the model parameters `penalty` and `mixture` for tuning

-   a tree model using the `ranger` engine; set the model parameters `min_n` and `trees` for tuning

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
#| eval: false
# PLEASE SHOW YOUR WORK


lm_mod_base <- 
  parsnip::linear_reg() %>% set_mode("regression") %>% set_engine("lm")
  
lm_mod_glmnet <- 
  parsnip::linear_reg(penalty = 0.1, mixture = 0.5) %>% set_mode("regression") %>% set_engine("glmnet")
  
lm_mod_rforest <- 
  parsnip::rand_forest(min_n = 5, trees = 100) %>% set_mode("regression") %>% set_engine("ranger")

```
:::

```{r}

```

## Exercise 5

Use parsnip::translate() on each model to see the code object that is specific to a particular engine

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
# PLEASE SHOW YOUR WORK

parsnip::linear_reg() %>% set_mode("regression") %>% set_engine("lm") %>%
  parsnip::translate()

parsnip::linear_reg(penalty = 0.1, mixture = 0.5) %>% set_mode("regression") %>% set_engine("glmnet") %>% parsnip::translate()

parsnip::rand_forest(min_n = 5, trees = 100) %>% 
  set_mode("regression") %>% set_engine("ranger") %>% 
  parsnip::translate()

```
:::

## Exercise 6 Bootstrap

Create bootstrap samples for the training dataset. You can leave the parameters set to their defaults

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
#| eval: false
# PLEASE SHOW YOUR WORK
set.seed(8740)
train_resamples <- ames_train %>% rsample::bootstraps()
```
:::

::: render-commit-push
This is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you've made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.
:::

## Exercise 7

Create workflows with `workflowsets::workflow_set` using your recipe and models.

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
#| eval: false
# PLEASE SHOW YOUR WORK

all_workflows <- 
  workflowsets::workflow_set(
    preproc = list(base = norm_recipe),
    models = list(base = lm_mod_base, glmnet = lm_mod_glmnet, forest = lm_mod_rforest)
  )
```
:::

## Exercise 8

Map the default function (`tune::tune_grid()`) across the workflows in the workflowset you just created and update the variable `all_workflows` with the result

```{r}
#| eval: false
all_workflows <- all_workflows %>% 
  workflowsets::workflow_map(
    verbose = TRUE                # enable logging
    , resamples = train_resamples # a parameter passed to tune::tune_grid()
    , grid = 5                    # a parameter passed to tune::tune_grid()
  )
```

The updated variable `all_workflows` contains a nested column named **result**, and each cell of the column **result** is a tibble containing a nested column named **.metrics**. Write code to

1.  un-nest the metrics in the column .metrics

2.  filter out the rows for the metric rmse

3.  group by wflow_id, order the .estimate column from highest to lowest, and pick out the first row of each group

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
#| eval: false
# PLEASE SHOW YOUR WORK

glimpse(all_workflows)

all_workflows %>%
  dplyr::select(wflow_id, result) %>%
  tidyr::unnest(result) %>%
  dplyr::select(wflow_id, .metrics) %>%
  tidyr::unnest(.metrics) %>%
  dplyr::filter(.metric == 'rmse') %>%
  dplyr::group_by(wflow_id) %>%
  dplyr::arrange(desc(.estimate)) %>%
  dplyr::slice(1)

```
:::

## Exercise 9

Run the code below and compare to your results from exercise 8.

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
#| eval: false
# PLEASE SHOW YOUR WORK

workflowsets::rank_results(all_workflows, rank_metric = "rmse", select_best = TRUE)

# compare to your results from exercise 8.
```
:::

## Exercise 10

Select the best model per the **rsme** metric using its id.

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
#| eval: false
# PLEASE SHOW YOUR WORK

glimpse(all_workflows)

best_model_workflow <- 
  all_workflows %>% 
  workflowsets::extract_workflow("base_forest")
```
:::

Finalize the workflow by setting the parameters for the best model

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
#| eval: false
# PLEASE SHOW YOUR WORK


best_model_workflow <- 
  best_model_workflow %>% 
  tune::finalize_workflow(
    tibble::tibble(trees = 100, min_n = 5) # enter the name and value of the best-fit parameters
  ) 
```
:::

Now compare the fits

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
#| eval: false
# PLEASE SHOW YOUR WORK

training_fit <- best_model_workflow %>% 
  fit(data = ames_train)

testing_fit <- best_model_workflow %>% 
  fit(data = ames_test)


# What is the ratio of the OOB prediction errors (MSE): test/train?

```
:::

::: render-commit-push
You're done and ready to submit your work! **Save**, **stage**, **commit**, and **push** all remaining changes. You can use the commit message "Done with Lab 4!", and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that **all** documents are updated in your repo on GitHub.
:::

::: callout-warning
## Submission

I will pull (copy) everyone's submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (**don't forget to commit and then push your work by 5:00pm on Sunday!**)
:::

## Grading

Total points available: 50 points.

| Component | Points |
|-----------|--------|
| Ex 1 - 10 | 30     |
