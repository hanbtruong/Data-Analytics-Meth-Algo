---
title: "Lab 3 - Regression"
subtitle: "BSMM 8740 Fall 2024"
author: "THI BAO HAN TRUONG"
format: html
editor: visual
self-contained: true
reference-location: margin
---

## Introduction

In today's lab, you'll explore several data sets and practice building and evaluating regression models.

### Learning goals

By the end of the lab you will...

-   Be able to use different regression models to predict a response/target/outcome as a function of a set of variates.

## Getting started

-   To complete the lab, log on to **your** github account and then go to the class [GitHub organization](https://github.com/bsmm-8740-fall-2024) and find the **2024-lab-3-\[your github username\]** repository to complete the lab.

    Create an R project using your **2024-lab-3-\[your github username\]** repository (remember to create a PAT, etc., as in lab-1) and add your answers by editing the `2024-lab-3.qmd` file in your repository.

-   When you are done, be sure to save your document, stage, commit and push your work.

::: callout-important
To access Github from the lab, you will need to make sure you are logged in as follows:

-   username: **.\\daladmin**
-   password: **Business507!**

Remember to (create a PAT and set your git credentials)

-   create your PAT using `usethis::create_github_token()` ,
-   store your PAT with `gitcreds::gitcreds_set()` ,
-   set your username and email with
    -   `usethis::use_git_config( user.name = ___, user.email = ___)`
:::

## Packages

We will use the following packages in today's lab.

```{r}
#| warning: false
#| message: false
# check if 'librarian' is installed and if not, install it
if (! "librarian" %in% rownames(installed.packages()) ){
  install.packages("librarian")
}
  
# load packages if not already loaded
librarian::shelf(
  tidyverse, magrittr, gt, gtExtras, tidymodels, DataExplorer, skimr, janitor, ggplot2, knitr,
  ISLR2, stats, xgboost, performance, dataexplorer
)
theme_set(theme_bw(base_size = 12))
```

## Data: Boston House Values

The Boston House Values dataset (usually referred to as the Boston dataset) appears in several R packages in different versions and is based on economic studies published in the late 1970's.

This dataset contains the following information for each cocktail:

| variable | description                                                            |
|----------|------------------------------------------------------------------------|
| crim     | per capita crime rate by town.                                         |
| zn       | proportion of residential land zoned for lots over 25,000 sq.ft.       |
| indus    | proportion of non-retail business acres per town.                      |
| chas     | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise). |
| nox      | nitrogen oxides concentration (parts per 10 million).                  |
| rm       | average number of rooms per dwelling.                                  |
| age      | proportion of owner-occupied units built prior to 1940.                |
| dis      | weighted mean of distances to five Boston employment centres.          |
| rad      | index of accessibility to radial highways.                             |
| tax      | full-value property-tax rate per \$10,000.                             |
| ptratio  | pupil-teacher ratio by town.                                           |
| lstat    | lower status of the population (percent).                              |
| medv     | median value of owner-occupied homes in \$1000s.                       |

Use the code below to load the Boston Cocktail Recipes data set.

```{r}
#| message: false
boston <- ISLR2::Boston
```

## Exercises

### Exercise 1

Plot the median value of owner-occupied homes (`medv`) vs the percentage of houses with lower socioeconomic status (`lstat`) then use `lm` to model `medv ~ lstat` and save the result in a variable for use later.

Next prepare a summary of the model. What is the intercept and the coefficient of `lstat` in this model?

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

What is the intercept and the coefficient of `lstat` in this model?

-   Intercept in this model: 34.55384

-   Coefficient of lstat: -0.95005

```{r}
# PLEASE SHOW YOUR WORK
# plot medv vs lstat
ggplot(Boston, aes(x = lstat, y = medv)) + 
  geom_point() + 
  labs(
    title = "medv vs lstat", x = "Percentage of Lower Status of Population", y = "Median Value of Homes"
  )
```

```{r}
# create a linear model of medv vs lstat and save the model
linear_model <- lm(medv ~ lstat, data = Boston)

# summarize the model
summary(linear_model)
```
:::

### Exercise 2

Using the result from Exercise 1, and the data below, use the predict function (`stats::predict.lm` or just `predict`) with the argument interval = "confidence" to prepare a summary table with columns *lstat*, *fit*, *lwr*, *upr*.

You can use `stats::predict.lm` directly with the data below.

Or consider creating a nested column using `dplyr::mutate` along with `purrr::map` with first argument *lstat* and second argument a function you create. The last operation is to unnest the nested column with `tidyr::unnest(__)`.

Or

```{r}
#| echo: true
#| eval: false
new_lstat <- tibble(lstat = c(5, 10, 15, 20))
```

Finally, use your model to plot some performance checks using the `performance::check_model` function with arguments `check=c("linearity","qq","homogeneity", "outliers")`.

Are there any overly influential observations in this dataset?

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
# PLEASESHOW YOUR WORK
# Use the predict function to get the predictions and confidence interval
predictions <- stats::predict.lm(linear_model, new_lstat, interval = c("confidence"))

predictions

# Combine the lstat values with the predictions
summary_table <- new_lstat %>% bind_cols(as_tibble(predictions))
summary_table

# Plot performance check
performance::check_model(linear_model, check=c("linearity","qq","homogeneity", "outliers"))

# Are there any overly influential observations in this dataset? yes
```
:::

### Exercise 3

Fit the variable `medv` (median value of owner-occupied homes) to all predictors in the dataset and use the `performance::check_collinearity` function on the resulting model to check if any predictors are redundant.

The variance inflation factor is a measure of the magnitude of multicollinearity of model terms. A VIF less than 5 indicates a low correlation of that predictor with other predictors. A value between 5 and 10 indicates a moderate correlation, while VIF values larger than 10 are a sign for high, not tolerable correlation of model predictors.

Which predictors in this dataset might be redundant for predicting `medv`?

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
# PLEASESHOW YOUR WORK
# Fit the variable medv (median value of owner-occupied homes) to all predictors in the dataset
model_all <- lm(medv~., data = Boston)

# Plot performance check
performance::check_collinearity(model_all)


# Which predictors in this dataset might be redundant for predicting `medv`? none of them.
```
:::

::: render-commit-push
This is a good place to **save**, **commit**, and **push** changes to your remote lab repo. Click the checkbox next to each file in the Git pane to stage the updates you've made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.
:::

### Exercise 4

In this exercise you will compare and interpret the results of linear regression on two similar datasets.

The first dataset (`dat0` - generated below) has `demand0` and `price0` variables along with an unobserved variable (`unobserved0` - so not in our dataset) that doesn't change the values of `demand0` and `price0`. Use `lm` to build a model to predict `demand0` from `price0` . Plot the data, including intercept and slope. What is the slope of the demand curve in dataset `dat0`?

::: callout-tip
plot with something like:

``` r
dat0 %>% ggplot(aes(x=price0,y=demand0)) + 
         # plot the points
         geom_point() +
         # add a straight line to the plot
         geom_abline(
          data = ?? a table with the coefficient estimates ??
            , aes(intercept = `(Intercept)`, slope = price0)
            , colour = "red"
         )
```
:::

```{r}
#| echo: true
#| eval: false
N <- 500
set.seed(1966)

dat0 <- tibble::tibble(
  price0 = 10+rnorm(500)
  , demand0 = 30-(price0 + rnorm(500))
  , unobserved0 = 0.45*price0 + 0.77*demand0 + rnorm(500)
)
dat0

```

The second dataset (`dat1` - generated below) has `demand1` and `price1` variables, along with a variable `unobserved1` that is completely random and is not observed, so it isn't in our dataset. Use lm to build a model to predict `demand1` from `price1` . Plot the data, including intercept and slope. What is the slope of the demand curve in dataset `dat1`?

```{r}
#| echo: true
#| eval: false
set.seed(1966)

dat1 <- tibble::tibble(
  unobserved1 = rnorm(500)
  , price1 = 10 + unobserved1 + rnorm(500)
  , demand1 = 23 -(0.5*price1 + unobserved1 + rnorm(500))
)
dat1
```

Which linear model returns the (approximately) correct dependence of demand on price, as given in the data generation process?

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
# PLEASESHOW YOUR WORK

# Build model to predict demand0 from price0
model_dat0 <- lm(demand0 ~ price0, data = dat0)

# Build model to predict demand1 from price1
model_dat1 <- lm(demand1 ~ price1, data = dat1)

# Extract the cofficient and create a dataframe
coefficients_0 <- data.frame(intercept = coef(model_dat0)[1], price0 = coef(model_dat0)[2])

coefficients_1 <- data.frame(intercept = coef(model_dat1)[1], price1 = coef(model_dat1)[2])

# Plot the data
dat0 %>% ggplot(aes(x = price0, y = demand0)) +
  geom_point() + 
  geom_abline(data = coefficients_0 , aes(intercept = intercept, slope = price0)
    , colour = "red"
  )
  
dat1 %>% ggplot(aes(x = price1, y = demand1)) +
  geom_point() +
  geom_abline(data = coefficients_1 , aes(intercept = intercept, slope = price1)
    , colour = "blue"
    )

summary(model_dat0)

summary(model_dat1)

# Which linear model returns the (approximately) correct dependence of demand on price, as given in the data generation process? -> Linear model_dat0, because the estimated coefficient is closer to true cofficient used in the data generation process


```
:::

## Exercise 5

Now repeat the modeling of exercise 4, but assuming that the formerly unobservable variables are now observable, and so can be included in the linear regression models.

Which model returns the (approximately) correct dependence of demand on price, as given in the data generation process?

What can you conclude from these two exercises?

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
# PLEASESHOW YOUR WORK
# Build model to predict demand0 from price0 and 
model_with_unobserved0 <- lm(demand0 ~ price0 + unobserved0, data = dat0)

# Build model to predict demand1 from price1
model_with_unobserved1 <- lm(demand1 ~ price1 + unobserved1, data = dat1)

summary(model_with_unobserved0)
summary(model_with_unobserved1)

# What can you conclude from these two exercises? 
# Linear model_dat1 returns the approximately correct independence of demand on price when we include unobserved1 in the model, because the its estimated coefficient is closer to true cofficient. 
# Unobversed plays a crucial role in explaining the variability in demand and should be included in modeling.

```
:::

### Exercise 6

For the next several exercises, we'll work with a new dataset. This dataset is taken from an [EPA](https://www.fueleconomy.gov/feg/download.shtml) site on fuel economy, in particular the fuel economy dataset for 2023.

Use the code below to load the FE Guide data set.

```{r}
#| echo: true
#| eval: false
dat <- 
  readxl::read_xlsx("data/2023 FE Guide for DOE-release dates before 7-28-2023.xlsx")
```

From the raw data in `dat`, we'll make a smaller dataset, and we'll need to do some cleaning to make it useable.

First select the columns "Comb FE (Guide) - Conventional Fuel", "Eng Displ",'\# Cyl', Transmission , "\# Gears", "Air Aspiration Method Desc", "Regen Braking Type Desc", "Batt Energy Capacity (Amp-hrs)" , "Drive Desc", "Fuel Usage Desc - Conventional Fuel", "Cyl Deact?", and "Var Valve Lift?" and then clean the column names using janitor::janitor::clean_names(). Assign the revised data to the variable `cars_23`.

Perform a quick check of the data using `DataExplorer::introduce()` and `DataExplorer::plot_missing()` and modify the data as follows

-   mutate the columns `comb_fe_guide_conventional_fuel`, `number_cyl`, and `number_gears` to ensure that they contain integers values, not doubles.
-   use `tidyr::replace_na` to replace any missing values in `batt_energy_capacity_amp_hrs` column with zeros, and replace and missing values in `regen_braking_type_desc` with empty strings ("").
-   finally, mutate the columns 'transmission','air_aspiration_method_desc','regen_braking_type_desc','drive_desc' ,'fuel_usage_desc_conventional_fuel','cyl_deact','var_valve_lift' so their values are factors.

Prepare a recipe to pre-process `cars_23` ahead of modelling, using `comb_fe_guide_conventional_fuel` as the outcome, with the following steps.

-   Centering for: recipes::all_numeric()
-   Scaling for: recipes::all_numeric()
-   Dummy variables from: recipes::all_factor()

How many predictor variables are there in `cars_23` ?

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
# PLEASESHOW YOUR WORK
# Modify the data
cars_23 <- dat |> select("Comb FE (Guide) - Conventional Fuel", "Eng Displ", "# Cyl", "Transmission" , "# Gears", "Air Aspiration Method Desc", "Regen Braking Type Desc", "Batt Energy Capacity (Amp-hrs)" , "Drive Desc", "Fuel Usage Desc - Conventional Fuel", "Cyl Deact?", "Var Valve Lift?") |> janitor::clean_names()
  
introduce(cars_23)

plot_missing(cars_23)

cars_23 <- cars_23 %>%
  mutate(
    comb_fe_guide_conventional_fuel = as.integer(comb_fe_guide_conventional_fuel),
    number_cyl = as.integer(number_cyl), number_gears = as.integer(number_gears)) |>
  mutate(
    batt_energy_capacity_amp_hrs = replace_na(batt_energy_capacity_amp_hrs, 0),
    regen_braking_type_desc = replace_na(regen_braking_type_desc, "")) |>
  mutate(
    transmission = as.factor(transmission), 
    air_aspiration_method_desc = as.factor(air_aspiration_method_desc), 
    regen_braking_type_desc = as.factor(regen_braking_type_desc), 
    drive_desc = as.factor(drive_desc), 
    fuel_usage_desc_conventional_fuel = as.factor(fuel_usage_desc_conventional_fuel), 
    cyl_deact = as.factor(cyl_deact), var_valve_lift = as.factor(var_valve_lift))

# Prepare a recipe to pre-process cars_23 ahead of modelling
cars_23_prepped <- 
  recipes::recipe(comb_fe_guide_conventional_fuel~., data = cars_23) %>% 
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_dummy(all_factor())

# How many predictor variables are there in `cars_23` ? 11 predictors

```
:::

### Exercise 7

For this exercise, set a sample size equal to 75% of the observations of `cars_23` and split the data as follows:

```{r}
#| echo: true
#| eval: false
set.seed(1966)

# sample 75% of the rows of the cars_23 dataset to make the training set
train <- cars_23 %>% 
  # make an ID column for use as a key
  tibble::rowid_to_column("ID") %>% 
  # sample the rows
  dplyr::sample_frac(0.75)

# remove the training dataset from the original dataset to make the training set
test  <- 
  dplyr::anti_join(
    cars_23 %>% tibble::rowid_to_column("ID") # add a key column to the original data
    , train
    , by = 'ID'
  )

# drop the ID column from training and test datasets
train %<>% dplyr::select(-ID); test %<>% dplyr::select(-ID)
```

Next prep the recipe created in the last exercise using `recipes::prep` on the training data, and then use the result of the prep step to `recipes::bake` with the training and test data. Save the baked data in separate variables for use later.

After these two steps how many columns are in the data? Why does this differ from the last step?

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
# PLEASESHOW YOUR WORK
# Prep the recipe
prepared_recipe <- prep(cars_23_prepped, training = train)

# Bake with train data
baked_train_data <- bake(prepared_recipe, new_data = train)

# Bake with test data
baked_test_data <- bake(prepared_recipe, new_data = test)

# After these two steps how many columns are in the data? Why does this differ from the last step?
# 45 columns. The number of columns in the baked data differs from the original data primarily due to the creation of dummy variables for categorical predictor. Each categorical variable is expanded into multiple binary (0/1) columns, increasing the total number of columns in the dataset.
```
:::

::: render-commit-push
This is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you've made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.
:::

### Exercise 8

In this exercise we will run xgboost::xgboost to evaluate the regression.

First run fit the model with default meta-parameters for `max_depth` and `eta`, using the training data per the code below:

```{r}
#| echo: true
#| eval: false
#| 
untuned_xgb <-
  xgboost::xgboost(
    data = baked_train_data %>% dplyr::select(-comb_fe_guide_conventional_fuel) %>% as.matrix(), 
    label = baked_train_data %>% dplyr::select(comb_fe_guide_conventional_fuel) %>% as.matrix(),
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 6,
    eta = .25
    , verbose = FALSE
  )
```

Next use the fitted model to predict the outcome using the test data:

```{r}
#| echo: true
#| eval: false
# create predictions using the test data and the fitted model
yhat <- predict(
  untuned_xgb
  , baked_test_data %>% 
    dplyr::select(-comb_fe_guide_conventional_fuel) %>% 
    as.matrix() 
)
```

Finally, pull out the `comb_fe_guide_conventional_fuel` column from the test data, assign it to the variable `y` and then use `caret::postResample` with arguments `yhat` and `y` to evaluate how well the model fits.

What is the RMSE for the un-tuned model?

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
# PLEASESHOW YOUR WORK
# Extract the comb_fe_guide_conventional_fuel column from the test data
y <- test$comb_fe_guide_conventional_fuel

# Evaluate
evaluation <- caret::postResample(pred = yhat, obs = y)


# What is the RMSE for the un-tuned model?
print(evaluation["RMSE"])
```
:::

## Exercise 9

In this exercise we are going to tune the model using cross validation. First we create a tuning grid for the parameters and then fit the model for all the values in the grid, saving the results.

Finally, we select the best parameters by least RMSE. This code will take a while to run

```{r}
#| echo: true
#| eval: false
#create hyperparameter grid
hyper_grid <- expand.grid(max_depth = seq(3, 6, 1), eta = seq(.2, .35, .01))  

# initialize our metric variables
xgb_train_rmse <- NULL
xgb_test_rmse  <- NULL

for (j in 1:nrow(hyper_grid)) {
  set.seed(123)
  m_xgb_untuned <- xgboost::xgb.cv(
    data = baked_train_data %>% dplyr::select(-comb_fe_guide_conventional_fuel) %>% as.matrix(), 
    label = baked_train_data %>% dplyr::select(comb_fe_guide_conventional_fuel) %>% as.matrix(),
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    nfold = 5,
    max_depth = hyper_grid$max_depth[j],
    eta = hyper_grid$eta[j],
    verbose = FALSE
  )
  
  xgb_train_rmse[j] <- m_xgb_untuned$evaluation_log$train_rmse_mean[m_xgb_untuned$best_iteration]
  xgb_test_rmse[j] <- m_xgb_untuned$evaluation_log$test_rmse_mean[m_xgb_untuned$best_iteration]
}    

best <- hyper_grid[which(xgb_test_rmse == min(xgb_test_rmse)),]; best # there may be ties
```

re-run the code from the last exercise and evaluate the fit using one of the best tuning parameters (i.e. when re-running the regression, set `max_depth` and `eta` to one pair the best-fit parameters \[there may be ties\]).

Is the tuned model better than the un-tuned model? If better, how much has the RMSE improved (in %).

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
# PLEASESHOW YOUR WORK

dtrain <- xgboost::xgb.DMatrix(data = baked_train_data %>% dplyr::select(-comb_fe_guide_conventional_fuel) %>% as.matrix(), 
                               label = baked_train_data$comb_fe_guide_conventional_fuel)

dtest <- xgboost::xgb.DMatrix(data = baked_test_data %>% dplyr::select(-comb_fe_guide_conventional_fuel) %>% as.matrix(), 
                              label = baked_test_data$comb_fe_guide_conventional_fuel)

# Re-run the model with the best hyperparameters
best_max_depth <- best$max_depth[1]
best_eta <- best$eta[1]


set.seed(123)
final_model <- xgboost::xgb.train(
  data = dtrain,
  nrounds = 1000,
  objective = "reg:squarederror",
  max_depth = best_max_depth,
  eta = best_eta,
  early_stopping_rounds = 3,
  watchlist = list(train = dtrain, test = dtest),
  verbose = FALSE
)

# Get the final RMSE on the test set
final_rmse <- sqrt(mean((predict(final_model, newdata = baked_test_data %>% dplyr::select(-comb_fe_guide_conventional_fuel) %>% as.matrix()) - baked_test_data$comb_fe_guide_conventional_fuel)^2))

# Assuming you have the RMSE of the untuned model
untuned_rmse <- 0.25  # Replace with the actual RMSE of the untuned model

# Calculate the improvement percentage
improvement_percentage <- ((untuned_rmse - final_rmse) / untuned_rmse) * 100

cat("Improvement in RMSE: ", improvement_percentage, "%")

# Is the tuned model better than the un-tuned model? If better, how much has the RMSE improved (in %).
```
:::

## Exercise 10

Using `xgboost::xgb.importance` rank the importance of each predictor in the model. Finally, take the top 10 predictors by importance and plot them using `xgboost::xgb.plot.importance`.

Per this model, what is the most important feature for predicting fuel efficiency?

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
# PLEASESHOW YOUR WORK
# Calculate feature importance
importance_matrix <- xgboost::xgb.importance(model = final_model)

# Plot the top 10 important features
xgboost::xgb.plot.importance(importance_matrix, top_n = 10)

# Identify the most important feature
most_important_feature <- importance_matrix[1, ]
most_important_feature


# What is the most important feature for predicting fuel efficiency? eng_displ
```
:::

::: render-commit-push
You're done and ready to submit your work! **Save**, **stage**, **commit**, and **push** all remaining changes. You can use the commit message "Done with Lab 3!" , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that **all** documents are updated in your repo on GitHub.
:::

::: render-commit-push
## Submission

I will pull (copy) everyone's submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (**don't forget to commit and then push your work by 5:00pm on Sunday!**)
:::

## Grading

Total points available: 30 points.

| Component | Points |
|-----------|--------|
| Ex 1 - 10 | 30     |
